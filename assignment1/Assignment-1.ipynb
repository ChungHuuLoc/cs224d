{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Softmax\n",
    "\n",
    "Prove that softmax is invariant to constant offsets in the input\n",
    "\n",
    "$softmax(x) = softmax(x + c)$\n",
    "\n",
    "$softmax(x)_i = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$\n",
    "\n",
    "*Proof.* $\\forall i \\in 1 ≤ i ≤ dim(x)$\n",
    "\n",
    "$softmax(x + c)_i = \\frac{e^{x_i + c}}{\\sum_j e^{x_j + c}}$\n",
    "\n",
    "$=\\frac{e^{x_i} e^c}{\\sum_j e^{x_j} e^c}$\n",
    "\n",
    "$=\\frac{e^c e^{x_i}}{e^c \\sum_j e^{x_j}}$\n",
    "\n",
    "$=\\frac{e^{x_i}}{\\sum_j e^{x_j}}$\n",
    "\n",
    "$=softmax(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derive Sigmoid\n",
    "\n",
    "$\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "$z = 1 + e^{-x}$\n",
    "\n",
    "$\\frac{d}{dx} \\sigma(x) = \\frac{d \\sigma}{dz} \\frac{1}{z} \\cdot \\frac{dz}{dx} z$\n",
    "\n",
    "$=\\frac{1}{z^2} \\frac{dz}{dx} z$\n",
    "\n",
    "$=\\frac{1}{z^2} \\cdot \\frac{dx}{dz} 1 + e^{-x}$\n",
    "\n",
    "$=\\frac{1}{z^2} \\cdot - e^{-x}$\n",
    "\n",
    "$=\\frac{-e^{-x}}{-(1 + e^{-x})^2}$\n",
    "\n",
    "$=\\frac{1}{1 + e^{-x}} \\frac{e^{-x}}{1 + e^{-x}}$\n",
    "\n",
    "$=\\sigma(x) \\frac{e^{-x}}{1 + e^{-x}}$\n",
    "\n",
    "$=\\sigma(x) \\frac{1 + e^{-x} - 1}{1 + e^{-x}}$\n",
    "\n",
    "$=\\sigma(x) \\left(\\frac{1 + e^{-x}}{1 + e^{-x}} - \\frac{1}{1 + e^{-x}} \\right)$\n",
    "\n",
    "$=\\sigma(x) \\left(1 - \\frac{1}{1 + e^{-x}} \\right)$\n",
    "\n",
    "$=\\sigma(x) (1 - \\sigma(x))$\n",
    "\n",
    "$=\\sigma(x) \\sigma(-x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Derive Cross-Entropy of Softmax\n",
    "$CE(y, \\hat y) = - \\sum_i y_i log(\\hat y_i)$\n",
    "\n",
    "$ \\hat y_i = \\frac{e^{x_i}}{\\sum_j e^{x_j}} $\n",
    "\n",
    "$\\frac{\\partial}{\\partial x_k} CE(y, \\hat y) = \n",
    "- \\frac{\\partial}{\\partial x_k} \\sum_i y_i log(\\hat y_i)$\n",
    "\n",
    "$= - \\frac{\\partial}{\\partial x_k} log(\\hat y_i)$\n",
    "\n",
    "$= - \\frac{\\partial}{\\partial x_k} log \\frac{e^{x_i}}{\\sum_j e^{x_j}} $\n",
    "\n",
    "$= - ( \\frac{\\partial}{\\partial x_k} \\log e^{x_i} - \\frac{\\partial}{\\partial x_k} \\log \\sum_j e^{x_j} ) $\n",
    "\n",
    "$= - ( \\frac{\\partial}{\\partial x_k} x_i - \\frac{\\partial}{\\partial x_k} \\log \\sum_j e^{x_j} ) $\n",
    "\n",
    "$= - ( \\frac{\\partial}{\\partial x_k} x_i - \\frac{\\partial}{\\partial x_k} \\log \\sum_j e^{x_j} ) $\n",
    "\n",
    "$= - ( \\frac{\\partial}{\\partial x_k} x_i - \\frac{1}{\\sum_j e^{x_j}} * \\frac{\\partial}{\\partial x_k} \\sum_j e^{x_j} ) $\n",
    "\n",
    "$= - ( \\frac{\\partial}{\\partial x_k} x_i - \\frac{1}{\\sum_j e^{x_j}} * \\frac{\\partial}{\\partial x_k} e^{x_k} ) $\n",
    "\n",
    "$= - ( \\frac{\\partial}{\\partial x_k} x_i - \\frac{e^{x_k}}{\\sum_j e^{x_j}}) $\n",
    "\n",
    "$= - ( \\frac{\\partial}{\\partial x_k} x_i - \\hat y_k) $\n",
    "\n",
    "$= \\hat y_k - \\frac{\\partial}{\\partial x_k} x_i $\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial}{\\partial x_k} CE(y, \\hat y) =\n",
    "    \\begin{cases}\n",
    "    \\hat y_k - 1 &\\text{if } x = k\n",
    "    \\hat y_k - 0 &\\text{if } x \\ne k\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "OR\n",
    "\n",
    "$\\frac{\\partial}{\\partial x} CE(y, \\hat y) = \\hat y - y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derive gradients wrt input to 3-layer NN\n",
    "\n",
    "$J = CE(y, \\hat y) = - \\sum_i y_i \\log \\hat y_i$\n",
    "\n",
    "$z_1 = xW_1 + b_1$\n",
    "\n",
    "$h = \\sigma(z_1)$\n",
    "\n",
    "$z_2 = hW_2 + b_2$\n",
    "\n",
    "$\\hat y = softmax(z_2)$\n",
    "\n",
    "$\\frac {\\partial}{\\partial x} CE(y, \\hat y)$\n",
    "\n",
    "$=\\frac {\\partial}{\\partial \\hat z_2} CE(y, \\hat y) * \\frac {\\partial z_2}{\\partial h} hW_2 + b * \\frac {\\partial h}{\\partial z_1} \\sigma(z_1) * \\frac {\\partial z_1} {\\partial x} xW_1 + b_1$\n",
    "\n",
    "$=(y - \\hat y) * W_2 * \\sigma(z_1)(1 - \\sigma(z_1)) * W_1$\n",
    "\n",
    "$=(y - \\hat y) * W_2 * \\sigma(xW_1 + b_1)(1 - \\sigma(xW_1 + b_1)) * W_1$\n",
    "\n",
    "OR\n",
    "\n",
    "$\\delta_1 = \\frac {\\partial}{\\partial \\hat z_2} CE(y, \\hat y) = y - \\hat y$\n",
    "\n",
    "$\\delta_2 = \\frac {\\partial CE}{\\partial h} = \\delta_1 \\frac {\\partial z_2}{\\partial h} = \\delta_1 W_2$\n",
    "\n",
    "$\\delta_3 = \\frac {\\partial CE}{\\partial z_1} = \\delta_2 \\frac {\\partial h}{\\partial z_1} = \\delta_2 \\sigma\\prime(z_1)$\n",
    "\n",
    "$\\frac {\\partial CE}{\\partial x} = \\delta_3 W_1^T$\n",
    "\n",
    "## Number of parameters\n",
    "If input is $D_x$-dimensional, output is $D_y$-dimensional, and there are $H$ hidden units, how many parameters?\n",
    "\n",
    "There should be $(D_x + 1) \\cdot H + (H + 1) \\cdot D_y)$ parameters."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
