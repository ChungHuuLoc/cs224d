{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Softmax\n",
    "\n",
    "Prove that softmax is invariant to constant offsets in the input\n",
    "\n",
    "$softmax(x) = softmax(x + c)$\n",
    "\n",
    "$softmax(x)_i = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$\n",
    "\n",
    "*Proof.* $\\forall i \\in 1 ≤ i ≤ dim(x)$\n",
    "\n",
    "$softmax(x + c)_i = \\frac{e^{x_i + c}}{\\sum_j e^{x_j + c}}$\n",
    "\n",
    "$=\\frac{e^{x_i} e^c}{\\sum_j e^{x_j} e^c}$\n",
    "\n",
    "$=\\frac{e^c e^{x_i}}{e^c \\sum_j e^{x_j}}$\n",
    "\n",
    "$=\\frac{e^{x_i}}{\\sum_j e^{x_j}}$\n",
    "\n",
    "$=softmax(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derive Sigmoid\n",
    "\n",
    "$\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "$z = 1 + e^{-x}$\n",
    "\n",
    "$\\frac{d}{dx} \\sigma(x) = \\frac{d \\sigma}{dz} \\frac{1}{z} \\cdot \\frac{dz}{dx} z$\n",
    "\n",
    "$=\\frac{1}{z^2} \\frac{dz}{dx} z$\n",
    "\n",
    "$=\\frac{1}{z^2} \\cdot \\frac{dx}{dz} 1 + e^{-x}$\n",
    "\n",
    "$=\\frac{1}{z^2} \\cdot - e^{-x}$\n",
    "\n",
    "$=\\frac{-e^{-x}}{-(1 + e^{-x})^2}$\n",
    "\n",
    "$=\\frac{1}{1 + e^{-x}} \\frac{e^{-x}}{1 + e^{-x}}$\n",
    "\n",
    "$=\\sigma(x) \\frac{e^{-x}}{1 + e^{-x}}$\n",
    "\n",
    "$=\\sigma(x) \\frac{1 + e^{-x} - 1}{1 + e^{-x}}$\n",
    "\n",
    "$=\\sigma(x) \\left(\\frac{1 + e^{-x}}{1 + e^{-x}} - \\frac{1}{1 + e^{-x}} \\right)$\n",
    "\n",
    "$=\\sigma(x) \\left(1 - \\frac{1}{1 + e^{-x}} \\right)$\n",
    "\n",
    "$=\\sigma(x) (1 - \\sigma(x))$\n",
    "\n",
    "$=\\sigma(x) \\sigma(-x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Derive Cross-Entropy of Softmax\n",
    "$CE(y, \\hat y) = - \\sum_i y_i log(\\hat y_i)$\n",
    "\n",
    "$ \\hat y_i = \\frac{e^{x_i}}{\\sum_j e^{x_j}} $\n",
    "\n",
    "$\\frac{\\partial}{\\partial x_k} CE(y, \\hat y) = \n",
    "- \\frac{\\partial}{\\partial x_k} \\sum_i y_i log(\\hat y_i)$\n",
    "\n",
    "$= - \\frac{\\partial}{\\partial x_k} log(\\hat y_i)$\n",
    "\n",
    "$= - \\frac{\\partial}{\\partial x_k} log \\frac{e^{x_i}}{\\sum_j e^{x_j}} $\n",
    "\n",
    "$= - ( \\frac{\\partial}{\\partial x_k} \\log e^{x_i} - \\frac{\\partial}{\\partial x_k} \\log \\sum_j e^{x_j} ) $\n",
    "\n",
    "$= - ( \\frac{\\partial}{\\partial x_k} x_i - \\frac{\\partial}{\\partial x_k} \\log \\sum_j e^{x_j} ) $\n",
    "\n",
    "$= - ( \\frac{\\partial}{\\partial x_k} x_i - \\frac{\\partial}{\\partial x_k} \\log \\sum_j e^{x_j} ) $\n",
    "\n",
    "$= - ( \\frac{\\partial}{\\partial x_k} x_i - \\frac{1}{\\sum_j e^{x_j}} * \\frac{\\partial}{\\partial x_k} \\sum_j e^{x_j} ) $\n",
    "\n",
    "$= - ( \\frac{\\partial}{\\partial x_k} x_i - \\frac{1}{\\sum_j e^{x_j}} * \\frac{\\partial}{\\partial x_k} e^{x_k} ) $\n",
    "\n",
    "$= - ( \\frac{\\partial}{\\partial x_k} x_i - \\frac{e^{x_k}}{\\sum_j e^{x_j}}) $\n",
    "\n",
    "$= - ( \\frac{\\partial}{\\partial x_k} x_i - \\hat y_k) $\n",
    "\n",
    "$= \\hat y_k - \\frac{\\partial}{\\partial x_k} x_i $\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial}{\\partial x_k} CE(y, \\hat y) =\n",
    "    \\begin{cases}\n",
    "    \\hat y_k - 1 &\\text{if } i = k \\\\[2ex]\n",
    "    \\hat y_k - 0 &\\text{if } i \\ne k\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "OR\n",
    "\n",
    "$\\frac{\\partial}{\\partial x} CE(y, \\hat y) = \\hat y - y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derive gradients wrt input to 3-layer NN\n",
    "\n",
    "$J = CE(y, \\hat y) = - \\sum_i y_i \\log \\hat y_i$\n",
    "\n",
    "$z_1 = xW_1 + b_1$\n",
    "\n",
    "$h = \\sigma(z_1)$\n",
    "\n",
    "$z_2 = hW_2 + b_2$\n",
    "\n",
    "$\\hat y = softmax(z_2)$\n",
    "\n",
    "$\\frac {\\partial}{\\partial x} CE(y, \\hat y)$\n",
    "\n",
    "$=\\frac {\\partial}{\\partial \\hat z_2} CE(y, \\hat y) * \\frac {\\partial z_2}{\\partial h} hW_2 + b * \\frac {\\partial h}{\\partial z_1} \\sigma(z_1) * \\frac {\\partial z_1} {\\partial x} xW_1 + b_1$\n",
    "\n",
    "$=(y - \\hat y) * W_2 * \\sigma(z_1)(1 - \\sigma(z_1)) * W_1$\n",
    "\n",
    "$=(y - \\hat y) * W_2 * \\sigma(xW_1 + b_1)(1 - \\sigma(xW_1 + b_1)) * W_1$\n",
    "\n",
    "OR\n",
    "\n",
    "$\\delta_3 = \\frac {\\partial}{\\partial \\hat z_2} CE(y, \\hat y) = y - \\hat y$\n",
    "\n",
    "$\\delta_2 = \\frac {\\partial CE}{\\partial h} = \\delta_1 \\frac {\\partial z_2}{\\partial h} = \\delta_1 W_2$\n",
    "\n",
    "$\\delta_1 = \\frac {\\partial CE}{\\partial z_1} = \\delta_2 \\frac {\\partial h}{\\partial z_1} = \\delta_2 \\sigma\\prime(z_1)$\n",
    "\n",
    "$\\frac {\\partial CE}{\\partial x} = \\delta_3 W_1^T$\n",
    "\n",
    "## Number of parameters\n",
    "If input is $D_x$-dimensional, output is $D_y$-dimensional, and there are $H$ hidden units, how many parameters?\n",
    "\n",
    "There should be $(D_x + 1) \\cdot H + (H + 1) \\cdot D_y)$ parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "## Derive gradients wrt $v_c$\n",
    "\n",
    "$J_{softmax-CE}(o, v_c, u_o) = - \\sum_i y_i \\log \\hat y = - \\log \\hat y$\n",
    "\n",
    "$\\hat y_o = p(o | c)$\n",
    "\n",
    "$z_{o,c} = u_o^T v_c$\n",
    "\n",
    "$z_c = U v_c$\n",
    "\n",
    "$softmax(z_c) = \\frac {exp(z_c)}{\\sum_{w=1}{W} (u_w^T v_c)}$\n",
    "\n",
    "Since $y_i = 0 \\forall i \\ne k$\n",
    "\n",
    "On individual elements:\n",
    "\n",
    "$\\frac {\\partial}{\\partial v_c} - \\log \\hat y$\n",
    "\n",
    "$= \\frac {\\partial}{\\partial v_c} - \\log softmax(z_{o,c})$\n",
    "\n",
    "$= \\frac {\\partial}{\\partial v_c} - \\log \\frac {\\exp(z_{o,c})}{\\sum_{w=1}{W} (u_w^T v_c)}$\n",
    "\n",
    "$= - [\\frac {\\partial}{\\partial v_c} \\log \\frac {\\exp(z_{o,c})}{\\sum_{w=1}{W} (u_w^T v_c)}]$\n",
    "\n",
    "$= - [\\frac {\\partial}{\\partial v_c} \\log exp(z_{o,c}) - \\frac {\\partial}{\\partial v_c} \\log \\sum_{w=1}^W (u_w^T v_c)]$\n",
    "\n",
    "$= \\frac {\\partial}{\\partial v_c} \\log \\sum_{w=1}^W \\exp(u_w^T v_c) - \\frac {\\partial}{\\partial v_c} \\log exp(z_{o,c})$\n",
    "\n",
    "$= \\frac {\\partial}{\\partial v_c} \\log \\sum_{w=1}^W \\exp(u_w^T v_c) - \\frac {\\partial}{\\partial v_c} z_{o,c}$\n",
    "\n",
    "$= \\frac {\\partial}{\\partial v_c} \\log \\sum_{w=1}^W \\exp(u_w^T v_c) - \\frac {\\partial}{\\partial v_c} z_{o,c}$\n",
    "\n",
    "$= \\frac {1}{\\sum_{w=1}^W \\exp(u_w^T v_c)} \\frac {\\partial}{\\partial v_c} \\sum_{x=1}^W \\exp(u_x^T v_c) - \\frac {\\partial}{\\partial v_c} z_{o,c}$\n",
    "\n",
    "$= \\sum_{x=1}^W \\frac {1}{\\sum_{w=1}^W \\exp(u_w^T v_c)}  \\frac {\\partial}{\\partial v_c} \\exp(u_x^T v_c) - \\frac {\\partial}{\\partial v_c} z_{o,c}$\n",
    "\n",
    "$= \\sum_{x=1}^W \\frac {1}{\\sum_{w=1}^W \\exp(u_w^T v_c)} \\exp(u_x^T v_c) \\cdot u_x - \\frac {\\partial}{\\partial v_c} z_{o,c}$\n",
    "\n",
    "$= \\sum_{x=1}^W \\frac {\\exp(u_x^T v_c)}{\\sum_{w=1}^W \\exp(u_w^T v_c)}  \\cdot u_x - \\frac {\\partial}{\\partial v_c} z_{o,c}$\n",
    "\n",
    "$= \\sum_{x=1}^W p(x|c) \\cdot u_x - \\frac {\\partial}{\\partial v_c} z_{o,c}$\n",
    "\n",
    "$= \\sum_{x=1}^W \\hat y_x \\cdot u_x - \\frac {\\partial}{\\partial v_c} u_o^T v_c$\n",
    "\n",
    "$= \\sum_{x=1}^W \\hat y_x \\cdot u_x - u_o$\n",
    "\n",
    "Equivalently, on vectors:\n",
    "\n",
    "$\\frac{\\partial}{\\partial v_c} J_{softmax-CE}(o, v_c, U)$\n",
    "\n",
    "$= \\frac{\\partial J}{\\partial z_c} \\frac{\\partial z_c}{\\partial v_c}$\n",
    "\n",
    "$= U^T (\\hat y - y)$\n",
    "\n",
    "## Derive gradients wrt all $u_w$ including $u_o$\n",
    "\n",
    "As before:\n",
    "\n",
    "$J_{softmax-CE}(c, v_c, u_o) = - \\sum_i y_i \\log \\hat y = - \\log \\hat y$\n",
    "\n",
    "Since $y_i = 0 \\forall i \\ne k$\n",
    "\n",
    "$\\hat y_o = p(o | c)$\n",
    "\n",
    "$z_{w,c} = u_w^T v_c$\n",
    "\n",
    "$softmax(z_{o,c}) = \\frac {exp(z_{o,c})}{\\sum_{w=1}{W} (u_w^T v_c)}$\n",
    "\n",
    "On individual elements:\n",
    "\n",
    "$\\frac {\\partial}{\\partial u_w} - \\log \\hat y$\n",
    "\n",
    "$= \\frac {\\partial}{\\partial u_w} - \\log softmax(z_{o,c})$\n",
    "\n",
    "$= \\frac {\\partial}{\\partial u_w} - \\log \\frac {\\exp(z_{o,c})}{\\sum_{w=1}{W} (u_w^T v_c)}$\n",
    "\n",
    "$= - [\\frac {\\partial}{\\partial u_w} \\log \\frac {\\exp(z_{o,c})}{\\sum_{w=1}{W} (u_w^T v_c)}]$\n",
    "\n",
    "$= - [\\frac {\\partial}{\\partial u_w} \\log exp(z_{o,c}) - \\frac {\\partial}{\\partial v_c} \\log \\sum_{w=1}^W (u_w^T v_c)]$\n",
    "\n",
    "$= \\frac {\\partial}{\\partial u_w} \\log \\sum_{w=1}^W \\exp(u_w^T v_c) - \\frac {\\partial}{\\partial u_w} \\log exp(z_{o,c})$\n",
    "\n",
    "$= \\frac {\\partial}{\\partial u_w} \\log \\sum_{w=1}^W \\exp(u_w^T v_c) - \\frac {\\partial}{\\partial u_w} z_{o,c}$\n",
    "\n",
    "$= \\frac {1}{\\sum_{w=1}^W \\exp(u_w^T v_c)} \\frac {\\partial}{\\partial u_w} \\sum_{x=1}^W \\exp(u_x^T v_c) - \\frac {\\partial}{\\partial u_w} z_{o,c}$\n",
    "\n",
    "The sum goes away since the derivative is 0 when $x \\ne w$\n",
    "\n",
    "$= \\frac {1}{\\sum_{w=1}^W \\exp(u_w^T v_c)} \\frac {\\partial}{\\partial u_w} \\exp(u_w^T v_c) - \\frac {\\partial}{\\partial u_w} z_{o,c}$\n",
    "\n",
    "$= \\frac {1}{\\sum_{w=1}^W \\exp(u_w^T v_c)} \\exp(u_w^T v_c) \\frac {\\partial}{\\partial u_w} u_w^T v_c - \\frac {\\partial}{\\partial u_w} z_{o,c}$\n",
    "\n",
    "$= \\frac {1}{\\sum_{w=1}^W \\exp(u_w^T v_c)} \\exp(u_w^T v_c) \\cdot v_c - \\frac {\\partial}{\\partial u_w} z_{o,c}$\n",
    "\n",
    "$= \\frac {\\exp(u_w^T v_c)}{\\sum_{w=1}^W \\exp(u_w^T v_c)} \\cdot v_c - \\frac {\\partial}{\\partial u_w} z_{o,c}$\n",
    "\n",
    "$= p(w | c) \\cdot v_c - \\frac {\\partial}{\\partial u_w} z_{o,c}$\n",
    "\n",
    "$= \\hat y_w \\cdot v_c - \\frac {\\partial}{\\partial u_w} z_{o,c}$\n",
    "\n",
    "$= \\hat y_w \\cdot v_c - \\frac {\\partial}{\\partial u_w} u_o^T v_c$\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial}{\\partial u_w} CE(y, \\hat y) =\n",
    "    \\begin{cases}\n",
    "    \\hat y_w \\cdot v_c - v_c = v_c(\\hat y_w - 1) &\\text{if } w = o \\\\[2ex]\n",
    "    \\hat y_w \\cdot v_c = v_c * \\hat y_w &\\text{if } w \\ne o\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Equivalently, on vectors:\n",
    "\n",
    "$\\frac{\\partial}{\\partial u_w} J_{softmax-CE}(v_c, u_w)$\n",
    "\n",
    "$= \\frac{\\partial J}{\\partial z_w} \\frac{\\partial z_w}{\\partial u_w}$\n",
    "\n",
    "$= v_c(\\hat y - y)^T $\n",
    "\n",
    "This is an outer product! $v_c$ is distributed across all the output word vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec with Negative Sampling\n",
    "\n",
    "## Derive gradients wrt $v_c$\n",
    "\n",
    "$J_{softmax-CE}(o, v_c, U) = - \\log (\\sigma (u_o^T v_c)) - \\sum_{k=1}^K \\log (\\sigma(-u_k^T v_c))$\n",
    "\n",
    "$\\frac{\\partial}{\\partial v_c} J = \\frac{\\partial}{\\partial v_c} - \\log \\sigma(u_o^T v_c) - \\sum_{k=1}^K \\log \\sigma (-u_k^T v_c)$\n",
    "\n",
    "$= - \\frac{\\partial}{\\partial v_c } \\log \\sigma(u_o^T v_c)\n",
    "- \\sum_{k=1}^K \\frac{\\partial}{\\partial v_c} \\log \\sigma (-u_k^T v_c)$\n",
    "\n",
    "$= - \\frac{1}{\\sigma(u_o^T v_c)} \\cdot \\sigma(u_o^T v_c) \\cdot (1 - \\sigma(u_o^T v_c)) \\cdot u_o\n",
    "- \\sum_{k=1}^K \\frac{1}{\\sigma (-u_k^T v_c)} \\cdot \\sigma (-u_k^T v_c) \\cdot (1 - \\sigma (-u_k^T v_c)) \\cdot -u_k$\n",
    "\n",
    "$= - (1 - \\sigma(u_o^T v_c)) u_o\n",
    "- \\sum_{k=1}^K (1 - \\sigma (-u_k^T v_c) -u_k$\n",
    "\n",
    "$= (\\sigma(u_o^T v_c) - 1) u_o\n",
    "- \\sum_{k=1}^K (\\sigma (-u_k^T v_c) - 1) u_k$\n",
    "\n",
    "## Derive gradients wrt all $u_k$ including $u_o$\n",
    "\n",
    "$J_{softmax-CE}(o, v_c, U) = - \\log (\\sigma (u_o^T v_c)) - \\sum_{k=1}^K \\log (\\sigma(-u_k^T v_c))$\n",
    "\n",
    "$\\frac{\\partial}{\\partial u_o} J = \\frac{\\partial}{\\partial u_o} - \\log \\sigma(u_o^T v_c) - \\sum_{k=1}^K \\log \\sigma (-u_k^T v_c)$\n",
    "\n",
    "$=- \\frac{\\partial}{\\partial u_o } \\log \\sigma(u_o^T v_c)\n",
    "- \\sum_{k=1}^K \\frac{\\partial}{\\partial u_o} \\log \\sigma (-u_k^T v_c)$\n",
    "\n",
    "$=- \\frac{\\partial}{\\partial u_o } \\log \\sigma(u_o^T v_c) - 0$\n",
    "\n",
    "$=- \\sigma(u_o^T v_c) \\frac{\\partial}{\\partial u_o } \\sigma(u_o^T v_c)$\n",
    "\n",
    "$=- \\sigma(u_o^T v_c) \\frac{\\partial}{\\partial u_o } \\sigma(u_o^T v_c)$\n",
    "\n",
    "$=- \\frac{1}{\\sigma(u_o^T v_c)} \\cdot \\sigma(u_o^T v_c) \\cdot (1 - \\sigma(u_o^T v_c)) \\cdot \\frac{\\partial}{\\partial u_o } u_o^T v_c$\n",
    "\n",
    "$=- \\cdot (1 - \\sigma(u_o^T v_c)) \\cdot v_c$\n",
    "\n",
    "$= (\\sigma(u_o^T v_c) - 1) v_c$\n",
    "\n",
    "$\\frac{\\partial}{\\partial u_k} J = \\frac{\\partial}{\\partial u_k} - \\log \\sigma(u_o^T v_c) - \\sum_{k=1}^K \\log \\sigma (-u_k^T v_c)$\n",
    "\n",
    "$= \\frac{\\partial}{\\partial u_k} - \\log \\sigma(u_o^T v_c) - \\frac{\\partial}{\\partial u_k} \\sum_{k=1}^K \\log \\sigma (-u_k^T v_c)$\n",
    "\n",
    "$= - \\frac{\\partial}{\\partial u_k} \\sum_{k=1}^K \\log \\sigma (-u_k^T v_c)$\n",
    "\n",
    "$= - \\frac{\\partial}{\\partial u_k} \\log \\sigma (-u_k^T v_c)$\n",
    "\n",
    "$= - \\frac{1}{\\sigma (-u_k^T v_c)} \\frac{\\partial}{\\partial u_k} \\sigma (-u_k^T v_c)$\n",
    "\n",
    "$= - \\frac{1}{\\sigma (-u_k^T v_c)} \\sigma (-u_k^T v_c) (1 - \\sigma (-u_k^T v_c)) \\frac{\\partial}{\\partial u_k} -u_k^T v_c$\n",
    "\n",
    "$= - (1 - \\sigma (-u_k^T v_c)) \\frac{\\partial}{\\partial u_k} -u_k^T v_c$\n",
    "\n",
    "$= - \\sigma (u_k^T v_c) \\frac{\\partial}{\\partial u_k} -u_k^T v_c$\n",
    "\n",
    "$= \\sigma (u_k^T v_c) v_c = (1 - \\sigma (-u_k^T v_c)) v_c $\n",
    "\n",
    "### Analysis\n",
    "\n",
    "Softmax CE Loss runs over the entire vocabulary, which is order N, whereas negative sampling only runs over the negative sampling size, which is order K. The speedup is approximately N/K."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-Gram Gradients\n",
    "\n",
    "$J_{softmax-CE}(o, v_c, U) = F(\\cdot)$\n",
    "\n",
    "$J_{skipgram}(word_{c-m...c+m}) = \\sum_{-m \\le j \\le m, j \\ne 0} F(w_{c+j}, v_c)$\n",
    "\n",
    "$\\frac{\\partial}{\\partial U} J = \\frac{\\partial}{\\partial U} \\sum_{-m \\le j \\le m, j \\ne 0} F(w_{c+j}, v_c)$\n",
    "\n",
    "$= \\sum_{-m \\le j \\le m, j \\ne 0} \\frac{\\partial  F(w_{c+j}, v_c)}{\\partial U}$\n",
    "\n",
    "where $\\frac{\\partial  F}{\\partial U}$ was defined as:\n",
    "\n",
    "$-v_c(\\hat y_w - 1)$ if $w = o$\n",
    "\n",
    "$-v_c^T \\hat y_w$ if $w \\ne o$\n",
    "\n",
    "$\\frac{\\partial}{\\partial v_c} J = \\frac{\\partial}{\\partial v_c} \\sum_{-m \\le j \\le m, j \\ne 0} F(w_{c+j}, v_c)$\n",
    "\n",
    "$= \\sum_{-m \\le j \\le m, j \\ne 0} \\frac{\\partial F(w_{c+j}, v_c)}{\\partial v_c} $\n",
    "\n",
    "where $\\frac{\\partial F}{\\partial v_c}$ was defined as:\n",
    "$U^T(y - \\hat y)$\n",
    "\n",
    "Finally:\n",
    "\n",
    "$\\frac{\\partial}{\\partial v_j} J = 0$\n",
    "\n",
    "$\\forall j \\ne c$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW Gradients\n",
    "\n",
    "$J_{softmax-CE}(o, v_c, U) = F(\\cdot)$\n",
    "\n",
    "$J_{CBOW}(word_{c-m...c+m}) = F(w_c, \\hat v)$\n",
    "\n",
    "where:\n",
    "\n",
    "$\\hat v = \\sum_{-m \\le j \\le m, j \\ne 0} v_{c+j}$\n",
    "\n",
    "$\\frac {\\partial J}{\\partial U} = \\frac {\\partial F(w_c, \\hat v)}{\\partial U}$\n",
    "\n",
    "$\\frac {\\partial J}{\\partial v_j} = \\frac {\\partial F(w_c, \\hat v)}{\\partial \\hat v}; \\forall j \\in {c - m, ... c - 1, c + 1, ..., c + m}$\n",
    "\n",
    "Note that all context words share a similar gradient since we averaged them and won't bother backpropogating their individual contributions. This is a bit less precise than SkipGram but, with a sufficiently large and diverse training set, tends to work pretty well and is a bit cheaper.\n",
    "\n",
    "$\\frac {\\partial J}{\\partial v_j} = 0; \\forall j \\notin {c - m, ... c - 1, c + 1, ..., c + m}$\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
